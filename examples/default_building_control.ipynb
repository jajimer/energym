{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false
            },
            "source": [
                "# Default building control setting up an empty action interface"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "When you want to run a simulation with all the default building control (specified in the building file). You can directly use the **EnergyPlus simulation engine**. For example, placing us in the workspace of the container would be to run the following:\n",
                "\n",
                "```bash\n",
                "$ energyplus -w sinergym/data/weather/USA_PA_Pittsburgh-Allegheny.County.AP.725205_TMY3.epw sinergym/data/buildings/5ZoneAutoDXVAV.epJSON\n",
                "```\n",
                "\n",
                "However, doing so without our framework has some **disadvantages**. You will have EnergyPlus default output and will not have some **added outputs** such as our wrapper logger that monitors all interactions with the environment. The buildings have a default *Site:Location* and *SizingPeriod:DesignDay*, which Sinergym changes automatically depending on the specified weather, so you would have to **adjust it** before using the simulator manually. Finally, the *RunPeriod* of the buildings are set to one episode for DRL, which means that as the buildings stand you can only simulate one year. So, you would also have to modify the *RunPeriod* **manually** in the building file before starting the simulation."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Therefore, our recommended proposal is setting up an **empty action interface** in an environment with Sinergym. For example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "is_executing": true,
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "#==============================================================================================#\n",
                        "\u001b[38;20m[ENVIRONMENT] (INFO) : Creating Gymnasium environment... [office-hot-continuous-v1]\u001b[0m\n",
                        "#==============================================================================================#\n",
                        "\u001b[38;20m[MODELING] (INFO) : Experiment working directory created [/workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res1]\u001b[0m\n",
                        "\u001b[38;20m[MODELING] (INFO) : runperiod established: {'start_day': 1, 'start_month': 1, 'start_year': 1991, 'end_day': 31, 'end_month': 12, 'end_year': 1991, 'start_weekday': 6, 'n_steps_per_hour': 4}\u001b[0m\n",
                        "\u001b[38;20m[MODELING] (INFO) : Episode length (seconds): 31536000.0\u001b[0m\n",
                        "\u001b[38;20m[MODELING] (INFO) : timestep size (seconds): 900.0\u001b[0m\n",
                        "\u001b[38;20m[MODELING] (INFO) : timesteps per episode: 35040\u001b[0m\n",
                        "\u001b[38;20m[MODELING] (INFO) : Model Config is correct.\u001b[0m\n",
                        "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment office-hot-continuous-v1 created successfully.\u001b[0m\n",
                        "\u001b[38;20m[WRAPPER LoggerWrapper] (INFO) : Wrapper initialized.\u001b[0m\n",
                        "#----------------------------------------------------------------------------------------------#\n",
                        "\u001b[38;20m[ENVIRONMENT] (INFO) : Starting a new episode... [Episode 1]\u001b[0m\n",
                        "#----------------------------------------------------------------------------------------------#\n",
                        "\u001b[38;20m[MODELING] (INFO) : Episode directory created [/workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res1/Eplus-env-sub_run1]\u001b[0m\n",
                        "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Variable available names\u001b[0m\n",
                        "\u001b[38;20m[ENVIRONMENT] (INFO) : Saving episode output path... [/workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res1/Eplus-env-sub_run1/output]\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/opyplus/weather_data/weather_data.py:493: FutureWarning: the 'line_terminator'' keyword is deprecated, use 'lineterminator' instead.\n",
                        "  epw_content = self._headers_to_epw(use_datetimes=use_datetimes) + df.to_csv(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m[SIMULATOR] (INFO) : Running EnergyPlus with args: ['-w', '/workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res1/Eplus-env-sub_run1/USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw', '-d', '/workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res1/Eplus-env-sub_run1/output', '/workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res1/Eplus-env-sub_run1/ASHRAE901_OfficeMedium_STD2019_Denver.epJSON']\u001b[0m\n",
                        "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 1 started.\u001b[0m\n",
                        "\u001b[38;20m[SIMULATOR] (INFO) : Handles initialized.\u001b[0m\n",
                        "\u001b[38;20m[SIMULATOR] (INFO) : Handles are ready.\u001b[0m\n",
                        "\u001b[38;20m[SIMULATOR] (INFO) : System is ready.\u001b[0m\n",
                        "\u001b[38;20m[WRAPPER LoggerWrapper] (INFO) : Creating monitor.csv for current episode (episode 1) if logger is active\u001b[0m\n",
                        "Reward:  -1.040098962863255 {'time_elapsed(hours)': 0.75, 'year': 2023, 'month': 7, 'day': 21, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 2, 'reward': -1.040098962863255, 'reward_energy': -2.037786761565728, 'reward_comfort': -0.04241116416078228, 'total_energy': 20377.86761565728, 'abs_comfort': 0.04241116416078228, 'temperatures': [26.738852781557213, 26.53822100691485, 26.379228760971404, 27.042411164160782, 26.70616176904293, 26.703710048030462, 26.721180581946832, 26.746080113592953, 26.724106409891995, 26.75203949912126, 26.148026393976973, 26.541555584377814, 26.325717463001574, 26.566761270651252, 26.717924954819775, 26.745565263920668, 26.720729493260777, 26.749034869196144]}\n",
                        "Reward:  -321.94333297935583 {'time_elapsed(hours)': 0.25, 'year': 2023, 'month': 12, 'day': 21, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 96, 'reward': -1.5044681734544123, 'reward_energy': -1.1473813114541747, 'reward_comfort': -1.86155503545465, 'total_energy': 11473.813114541746, 'abs_comfort': 1.86155503545465, 'temperatures': [28.715350456303796, 26.409077615767682, 26.302413730101993, 26.71821213417214, 26.230010119525883, 26.22754074805396, 26.44042888947091, 26.656050957931974, 26.48727140550084, 26.695129206819427, 26.642617651433046, 27.051399500650405, 26.756031063682798, 27.09480507850045, 26.3943238559042, 26.673621464207322, 26.442441479909714, 26.693560859167892]}\n",
                        "Reward:  -8449.524452766622 {'time_elapsed(hours)': 0.25, 'year': 2003, 'month': 1, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 1056, 'reward': -15.861740792564312, 'reward_energy': -0.3527765126687401, 'reward_comfort': -31.370705072459884, 'total_energy': 3527.765126687401, 'abs_comfort': 31.370705072459884, 'temperatures': [19.420825188480293, 14.778404487848041, 17.290130409903544, 18.129138457616943, 18.58653390450303, 16.886551558424042, 15.599302899168238, 15.395203253141759, 15.599392547994475, 15.286946996304188, 16.420519653080085, 15.704103829301099, 16.256071375290826, 15.669206183835808, 16.20332796285531, 15.712274304701937, 16.10856465471715, 15.719294810973615]}\n",
                        "Reward:  -10750.440574700875 {'time_elapsed(hours)': 744.25, 'year': 2005, 'month': 2, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 5280, 'reward': -0.012464598401376701, 'reward_energy': -0.024929196802753403, 'reward_comfort': -0.0, 'total_energy': 249.291968027534, 'abs_comfort': 0.0, 'temperatures': [26.21912426504149, 23.095761889231504, 25.160135664161377, 25.362835055887604, 25.917356898161536, 25.285018051933495, 23.83693968636221, 24.011764986384435, 24.741099096986893, 24.356644056884797, 23.782854159839175, 24.522285749506118, 26.13700576287969, 24.887360286052022, 24.346272157276392, 24.514616350009167, 25.234826442130952, 24.843366667436896]}\n",
                        "Reward:  -11548.310161293475 {'time_elapsed(hours)': 1416.25, 'year': 1995, 'month': 3, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 7968, 'reward': -0.012464598401376701, 'reward_energy': -0.024929196802753403, 'reward_comfort': -0.0, 'total_energy': 249.291968027534, 'abs_comfort': 0.0, 'temperatures': [26.11289088035044, 22.937592868558674, 25.068457617061277, 25.214515536182432, 25.825961202011428, 25.172935548467365, 23.614219524308183, 23.881546458396002, 24.68815974170161, 24.323307395019906, 23.358721329197138, 24.376881206065555, 25.699528017000905, 24.805278122032117, 24.140550519424735, 24.396542675640383, 25.195966805748768, 24.825497690282578]}\n",
                        "Reward:  -13024.716635645265 {'time_elapsed(hours)': 2160.25, 'year': 2002, 'month': 4, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 10944, 'reward': -0.010059948897039251, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.0, 'total_energy': 201.198977940785, 'abs_comfort': 0.0, 'temperatures': [26.101809520631015, 23.40368853247839, 25.142168040134862, 25.17566543208235, 25.701724843211878, 25.285285516062352, 23.959074802556867, 24.562209512070677, 24.936360874771722, 25.001521895586173, 23.12959387519747, 24.768420504192463, 25.306560918365463, 24.975432705586524, 24.26206778967132, 24.891104991180597, 25.296997432765682, 25.373554185022012]}\n",
                        "Reward:  -16892.39330533204 {'time_elapsed(hours)': 2880.25, 'year': 2004, 'month': 5, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 13824, 'reward': -0.010059948897039251, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.0, 'total_energy': 201.198977940785, 'abs_comfort': 0.0, 'temperatures': [26.364304745707, 24.52353100131502, 25.832212197832018, 25.823868428841223, 26.223149614256528, 25.910775586048473, 25.221807867239765, 25.54309902949941, 25.555192120971242, 26.037631952045313, 24.917411478361856, 25.81976993191239, 25.663060986093697, 26.275803704105005, 25.464309633032688, 25.758644981676184, 25.77979346351664, 26.273985202836723]}\n",
                        "Reward:  -21896.480693797734 {'time_elapsed(hours)': 3624.25, 'year': 2003, 'month': 6, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 16800, 'reward': -0.03656761090527819, 'reward_energy': -0.01686090046831275, 'reward_comfort': -0.05627432134224364, 'total_energy': 168.6090046831275, 'abs_comfort': 0.05627432134224364, 'temperatures': [26.15140360671308, 25.223731837093776, 25.983802367306314, 25.898260934738033, 26.088978318786456, 25.879303281456032, 26.00572531774445, 26.283128298930137, 25.990452649848006, 26.943730786516944, 25.44940009667066, 26.423375515019973, 25.55652147904463, 27.056274321342244, 26.08882880119884, 26.347169842413564, 26.06528628506544, 26.97647127236182]}\n",
                        "Reward:  -29220.265527102845 {'time_elapsed(hours)': 4344.25, 'year': 2004, 'month': 7, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 19680, 'reward': -1.2216384486365506, 'reward_energy': -1.055048725212252, 'reward_comfort': -1.388228172060849, 'total_energy': 10550.487252122519, 'abs_comfort': 1.388228172060849, 'temperatures': [25.20145692663023, 27.295373713699583, 27.293011769506034, 26.723837597769283, 26.049697371208428, 26.25091051950658, 26.5725978390231, 26.702160445069826, 26.606850179611964, 26.706310412899487, 25.708074853301696, 26.700906146955763, 25.81101776778896, 26.70405351489032, 26.89990196458047, 27.262119610043253, 26.927443517481343, 27.53772307881198]}\n",
                        "Reward:  -35568.615216710044 {'time_elapsed(hours)': 5088.25, 'year': 2002, 'month': 8, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 22656, 'reward': -0.12869784832509096, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.2372757988561034, 'total_energy': 201.198977940785, 'abs_comfort': 0.2372757988561034, 'temperatures': [25.80648362075324, 26.147287982162815, 26.09979271764262, 26.043162058666713, 26.1643436440468, 25.785023790360874, 26.33270897175462, 26.901520882519865, 26.568690918158726, 27.066793612183282, 25.741493187643748, 26.971982540585078, 26.314056970303113, 27.00955954478617, 26.582103777670657, 26.97985297049849, 26.7730220961208, 27.16092264188665]}\n",
                        "Reward:  -41555.36359713831 {'time_elapsed(hours)': 5832.25, 'year': 1994, 'month': 9, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 25632, 'reward': -0.008430450234156376, 'reward_energy': -0.01686090046831275, 'reward_comfort': -0.0, 'total_energy': 168.6090046831275, 'abs_comfort': 0.0, 'temperatures': [26.21107242792546, 25.838355247327563, 25.938764561998145, 25.900157283912684, 26.181329739987554, 26.240170465664086, 26.380860567849055, 26.544520933256333, 26.533415639221538, 26.61684155327248, 25.915470976652763, 26.57458377997023, 26.45888357067811, 26.56778071345284, 26.327116306844424, 26.506560915224213, 26.47460199399022, 26.56847947253168]}\n",
                        "Reward:  -46726.07328473001 {'time_elapsed(hours)': 6552.25, 'year': 1997, 'month': 10, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 28512, 'reward': -0.07082977021238261, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.12153964263068673, 'total_energy': 201.198977940785, 'abs_comfort': 0.12153964263068673, 'temperatures': [26.53158043045902, 25.48647480158813, 26.037562600855964, 26.115601295511542, 26.39286930416965, 26.20661087980376, 26.11313456302384, 26.304625097123846, 26.37339449404025, 26.329425930746083, 26.22588585079181, 26.866360873716157, 27.121539642630687, 26.763409612435147, 26.272436341185614, 26.4574432468797, 26.533984321803523, 26.4900801558824]}\n",
                        "Reward:  -50168.86575368962 {'time_elapsed(hours)': 7296.25, 'year': 2003, 'month': 11, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 31488, 'reward': -0.14016843470906087, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.2602169716240432, 'total_energy': 201.198977940785, 'abs_comfort': 0.2602169716240432, 'temperatures': [26.17131592008077, 23.552675907792565, 25.43103559207787, 25.629959968805373, 26.000287048043262, 25.36170312583189, 24.24540226798956, 24.449484070066617, 25.327159202066664, 24.810844684844092, 24.587782770752213, 25.380977105308588, 27.260216971624043, 25.780143053480415, 24.75174577320524, 24.95831757024713, 25.847453785728124, 25.310176939425926]}\n",
                        "Reward:  -52076.89194470986 {'time_elapsed(hours)': 8016.25, 'year': 2005, 'month': 12, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 34368, 'reward': -0.008430450234156376, 'reward_energy': -0.01686090046831275, 'reward_comfort': -0.0, 'total_energy': 168.6090046831275, 'abs_comfort': 0.0, 'temperatures': [25.85678538013335, 22.12365136581806, 24.715268001414902, 25.14953464387099, 25.482333985270447, 24.452463651285846, 22.745669402529554, 22.95370320536375, 23.898150640751346, 23.056023307832053, 23.473468536663706, 24.321186564294635, 26.523831518567764, 24.28969794995208, 23.560203396547244, 23.769711514500692, 24.71262840609815, 23.860393908790915]}\n",
                        "Progress: |****************************************************************************************************| 100%\n",
                        "Episode  0 Mean reward:  -1.4328161223963918 Cumulative reward:  -53505.652458642195\n",
                        "\u001b[38;20m[WRAPPER LoggerWrapper] (INFO) : End of episode, recording summary (progress.csv) if logger is active\u001b[0m\n",
                        "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment closed.\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "import gymnasium as gym\n",
                "import numpy as np\n",
                "\n",
                "import sinergym\n",
                "from sinergym.utils.wrappers import LoggerWrapper\n",
                "\n",
                "env = gym.make(\n",
                "    'Eplus-office-hot-continuous-v1',\n",
                "    actuators={},\n",
                "    action_space=gym.spaces.Box(\n",
                "        low=0,\n",
                "        high=0,\n",
                "        shape=(0,)))\n",
                "env = LoggerWrapper(env)\n",
                "\n",
                "for i in range(1):\n",
                "    obs, info = env.reset()\n",
                "    rewards = []\n",
                "    terminated = False\n",
                "    current_month = 0\n",
                "    while not terminated:\n",
                "        a = env.action_space.sample()\n",
                "        obs, reward, terminated, truncated, info = env.step(a)\n",
                "        rewards.append(reward)\n",
                "        if info['month'] != current_month:  # display results every month\n",
                "            current_month = info['month']\n",
                "            print('Reward: ', sum(rewards), info)\n",
                "    print(\n",
                "        'Episode ',\n",
                "        i,\n",
                "        'Mean reward: ',\n",
                "        np.mean(rewards),\n",
                "        'Cumulative reward: ',\n",
                "        sum(rewards))\n",
                "env.close()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this example, a default environment is loaded, but the space and definition of the default action **is replaced with an empty one**, Sinergym takes care of making the necessary changes in the background. Then, the random agent implemented send **empty actions ([])**.\n",
                "\n",
                "The advantages are that you can **combine the weathers with the buildings** as you want, Sinergym will take care of adapting everything automatically (you don't have the disadvantages of before), you can run in a single experiment as many years as you want, with our loggers. When you set an empty action interface, Sinergym preserves the default actuators that the building definition comes with (these can be more or less sophisticated depending on the definition of the building in the *epJSON* file)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.4 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "vscode": {
            "interpreter": {
                "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
